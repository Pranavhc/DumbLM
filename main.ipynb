{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a2fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffceaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # pad token at end of sentence \n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e61491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 40000 examples [00:00, 2407164.73 examples/s]\n",
      "Map: 100%|██████████| 40000/40000 [00:00<00:00, 64835.65 examples/s]\n",
      "Map: 100%|██████████| 40000/40000 [00:01<00:00, 28758.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"input.txt\"})\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True)\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = (len(concatenated[\"input_ids\"]) // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd588e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.00,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    " )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df67b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"saved_model\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"./saved_model_quantized\")\n",
    "tokenizer.save_pretrained(\"./saved_model_quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09520e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "model.from_pretrained(\"./saved_model_quantized\")\n",
    "tokenizer.from_pretrained(\"./saved_model_quantized\")\n",
    "print(\"Model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc1a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " one plus one equals one for the rest of the body. In fact, it is possible to change your position in the head by simply removing the head from the body, and then changing the position of one part of the body to another. This can be done by moving your hands slowly, or by twisting your arms, legs, or neck or by pulling your shoulders. This technique can also be performed with a flat object such as a finger.\n",
      "\n",
      "For a more detailed explanation of how to create a head-in\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input_ids = tokenizer(\"one plus one equals\", return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**input_ids, cache_implementation=\"static\", max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "print(\"\\n\\n\",tokenizer.decode(output[0], skip_special_tokens=True) df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
